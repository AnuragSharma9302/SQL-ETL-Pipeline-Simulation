# ğŸ§© SQL ETL Pipeline Simulation  

![PostgreSQL](https://img.shields.io/badge/Database-PostgreSQL-blue?logo=postgresql)
![pgAdmin](https://img.shields.io/badge/Tool-pgAdmin-orange?logo=postgresql)
![License](https://img.shields.io/badge/License-MIT-green)
![Dataset](https://img.shields.io/badge/Data-Kaggle%20E--commerce-lightgrey?logo=kaggle)

---

### ğŸ¯ **Overview**
A compact **SQL-based ETL Pipeline** built using **PostgreSQL** and **pgAdmin**, designed to simulate a real-world data engineering workflow â€” from importing raw CSV data to transforming, cleaning, and loading it into production-ready tables with audit logging and automation.

---

### âš™ï¸ **How It Works**
1. **Extract** â†’ Imported Kaggle E-commerce CSV into PostgreSQL staging tables using the `COPY` command.  
2. **Transform** â†’ Cleaned and standardized data (handled nulls, duplicates, invalid prices).  
3. **Load** â†’ Moved transformed data to production tables with constraints.  
4. **Audit** â†’ Implemented triggers and audit logs to track every ETL operation.  
5. **Validate** â†’ Wrote SQL scripts to verify row counts, data quality, and totals.

---

## ğŸ§° Project Structure  

```bash
SQL-ETL-Pipeline-Simulation/
â”‚
â”œâ”€â”€ ğŸ“ data/               # Raw Kaggle dataset (CSV files)
â”‚   â””â”€â”€ ecommerce_data.csv
â”‚
â”œâ”€â”€ ğŸ“ scripts/            # SQL scripts used for ETL (DDL, DML, triggers, audits)
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”œâ”€â”€ transform_data.sql
â”‚   â”œâ”€â”€ load_production.sql
â”‚   â””â”€â”€ audit_triggers.sql
â”‚
â”œâ”€â”€ ğŸ“ exports/            # Final exported cleaned & validated production tables
â”‚   â”œâ”€â”€ production_customers.csv
â”‚   â”œâ”€â”€ production_orders.csv
â”‚   â””â”€â”€ etl_logs.csv
â”‚
â”œâ”€â”€ ğŸ“ validation/         # SQL queries for verification & data quality checks
â”‚   â””â”€â”€ etl_validation_queries.sql
â”‚
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ LICENSE                # Open-source license
```
---
### ğŸ“¦ Folder Breakdown
Folder	Description
data/	      :  Contains raw Kaggle datasets used as input for ETL.
scripts/	  : Holds SQL scripts for table creation, transformation, loading, and automation                     (triggers, procedures).
exports/	  : Includes final cleaned and production-ready tables exported from PostgreSQL.
validation/ : Contains SQL validation and testing queries for verifying ETL results.

---

### ğŸ’¡ **Why It Matters**
This project demonstrates how to manage **ETL workflows entirely in SQL**, showcasing:
- Handling **real-world data issues** (encoding, nulls, outliers).  
- Applying **data quality checks and audit trails**.  
- Building maintainable, modular ETL pipelines using only PostgreSQL.  

---

### ğŸ§  **Key Learnings**
- Practical experience with **SQL-based data engineering**.  
- Built triggers, audit tables, and cleaning logic in **PL/pgSQL**.  
- Learned to troubleshoot **encoding and import errors** (`WIN1252 â†’ UTF8`).  
- Improved skills in **data modeling** and **ETL validation queries**.

---

### ğŸš€ **Outcomes**
âœ… Built a **fully functional ETL pipeline** in SQL.  
âœ… Produced **cleaned, validated production tables** from raw CSVs.  
âœ… Created **automated audit logging** for data loads.  
âœ… Achieved **complete traceability and reproducibility** of ETL steps.  

---

### ğŸ§° **Tech Stack**
**PostgreSQL 17** â€¢ **pgAdmin 4** â€¢ **SQL** â€¢ **Kaggle E-commerce Dataset**

---

### ğŸŒ± **Next Steps**
- Add **Python / Streamlit dashboard** to visualize ETL metrics.  
- Automate ETL execution via **Airflow** or **cron jobs**.  
- Extend with **error handling and notifications** for failed loads.  

---

> ğŸ—£ *â€œThis project helped me bridge SQL fundamentals with real-world data engineering â€” building pipelines that are clean, auditable, and production-ready.â€*

---
